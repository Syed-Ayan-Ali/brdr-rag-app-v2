#!/usr/bin/env tsx

import { config } from 'dotenv';
import { markdownPageChunker } from '../lib/chunking/MarkdownPageChunker';
import { logger, LogCategory } from '../lib/logging/Logger';

// Load environment variables
config();

async function testMarkdownChunking() {
  console.log('ğŸ§ª Testing Markdown Page Chunking...');

  try {
    // Step 1: Get collection statistics
    console.log('\nğŸ“Š Step 1: Getting collection statistics...');
    const stats = markdownPageChunker.getCollectionStats();
    console.log('ğŸ“ˆ Collection Statistics:');
    console.log(`  ğŸ“ Total markdown files: ${stats.totalFiles}`);
    console.log(`  ğŸ“„ Total documents: ${stats.totalDocuments}`);
    console.log(`  ğŸ“ Total pages: ${stats.totalPages}`);
    console.log(`  ğŸ“Š Average pages per document: ${stats.avgPagesPerDocument}`);

    if (stats.totalFiles === 0) {
      console.log('âŒ No markdown files found in public/brdr-md');
      console.log('ğŸ’¡ Please ensure your markdown files are in the correct directory');
      return;
    }

    // Step 2: Test parsing a few sample documents
    console.log('\nğŸ” Step 2: Testing document parsing...');
    const files = markdownPageChunker.getAllMarkdownFiles();
    const sampleFiles = files.slice(0, 3); // Test first 3 files

    for (const filename of sampleFiles) {
      console.log(`\nğŸ“„ Processing: ${filename}`);
      try {
        const document = markdownPageChunker.parseMarkdownFile(filename);
        if (document) {
          console.log(`  âœ… Successfully parsed`);
          console.log(`  ğŸ“ Title: ${document.title}`);
          console.log(`  ğŸ‘¤ Author: ${document.author || 'N/A'}`);
          console.log(`  ğŸ“… Creation Date: ${document.creationDate || 'N/A'}`);
          console.log(`  ğŸ“Š Pages: ${document.pages.length}`);
          
          // Show sample page
          if (document.pages.length > 0) {
            const firstPage = document.pages[0];
            console.log(`  ğŸ“‹ Sample Page ${firstPage.pageNumber}:`);
            console.log(`    ğŸ“ Length: ${firstPage.cleanContent.length} chars`);
            console.log(`    ğŸ”¤ Tokens: ${firstPage.metadata.tokens}`);
            console.log(`    ğŸ·ï¸ Keywords: ${firstPage.metadata.keywords.slice(0, 3).join(', ')}...`);
            console.log(`    ğŸ“ Preview: ${firstPage.cleanContent.substring(0, 100)}...`);
          }
        } else {
          console.log(`  âŒ Failed to parse`);
        }
      } catch (error) {
        console.log(`  âŒ Error: ${error}`);
      }
    }

    // Step 3: Test full processing
    console.log('\nâš™ï¸ Step 3: Testing full document processing...');
    const processedDocs = markdownPageChunker.processAllDocuments();
    
    console.log(`âœ… Successfully processed ${processedDocs.length} documents`);
    
    if (processedDocs.length > 0) {
      const totalChunks = processedDocs.reduce((sum, doc) => sum + doc.chunks.length, 0);
      const avgChunksPerDoc = totalChunks / processedDocs.length;
      
      console.log(`ğŸ“Š Processing Summary:`);
      console.log(`  ğŸ“„ Documents: ${processedDocs.length}`);
      console.log(`  ğŸ§© Total chunks: ${totalChunks}`);
      console.log(`  ğŸ“ˆ Average chunks per document: ${avgChunksPerDoc.toFixed(1)}`);
      
      // Show sample processed document
      const sampleDoc = processedDocs[0];
      console.log(`\nğŸ“„ Sample Processed Document: ${sampleDoc.docId}`);
      console.log(`  ğŸ“ Title: ${sampleDoc.title}`);
      console.log(`  ğŸ“Š Chunks: ${sampleDoc.chunks.length}`);
      console.log(`  ğŸ·ï¸ Topics: ${sampleDoc.metadata.source}`);
      console.log(`  ğŸ“ Content length: ${sampleDoc.fullContent.length} chars`);
      
      // Show chunk details
      if (sampleDoc.chunks.length > 0) {
        console.log(`\nğŸ§© Sample Chunks:`);
        sampleDoc.chunks.slice(0, 2).forEach((chunk, index) => {
          console.log(`  ğŸ“‹ Chunk ${index + 1} (Page ${chunk.pageNumber}):`);
          console.log(`    ğŸ“ Length: ${chunk.cleanContent.length} chars`);
          console.log(`    ğŸ”¤ Tokens: ${chunk.metadata.tokens}`);
          console.log(`    ğŸ·ï¸ Keywords: ${chunk.metadata.keywords.slice(0, 3).join(', ')}`);
          console.log(`    ğŸ“ Preview: ${chunk.cleanContent.substring(0, 80)}...`);
        });
      }
    }

    // Step 4: Performance test
    console.log('\nâš¡ Step 4: Performance test...');
    const startTime = Date.now();
    markdownPageChunker.processAllDocuments();
    const endTime = Date.now();
    const processingTime = endTime - startTime;
    
    console.log(`â±ï¸ Processing time: ${processingTime}ms`);
    console.log(`ğŸ“Š Performance: ${(stats.totalDocuments / (processingTime / 1000)).toFixed(1)} documents/second`);

    console.log('\nğŸ‰ Markdown chunking test completed successfully!');
    console.log('âœ¨ The chunking system is ready for the ETL pipeline.');

  } catch (error) {
    console.error('\nâŒ Markdown chunking test failed:', error);
    logger.error(LogCategory.CHUNKING, 'Chunking test failed', error);
    process.exit(1);
  }
}

// Execute if run directly
if (require.main === module) {
  testMarkdownChunking().catch(console.error);
}

export { testMarkdownChunking };
